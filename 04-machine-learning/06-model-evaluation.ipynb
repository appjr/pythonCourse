{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 ML: Model Evaluation\n\n---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\ny_true = [0, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 0, 1]\n\nprint(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\nprint(f\"Precision: {precision_score(y_true, y_pred)}\")\nprint(f\"Recall: {recall_score(y_true, y_pred)}\")\nprint(f\"F1 Score: {f1_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Key Takeaways\n\u2705 Multiple metrics needed\n\u2705 Cross-validation prevents overfitting\n\u2705 Grid search tunes hyperparameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}